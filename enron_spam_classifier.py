# -*- coding: utf-8 -*-
"""enron_spam_classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/gist/anewcheng/81d9ec581bcfb7aa0c38bf13454de4d0/enron_spam_classifier.ipynb
"""

# ========================================
# Enron-Spam Dataset 垃圾郵件分類專案
# ========================================

# 1. 匯入必要的函式庫
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                            f1_score, confusion_matrix, classification_report,
                            roc_auc_score, roc_curve)
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# 下載 stopwords (首次執行需要)
nltk.download('stopwords')

# 2. 載入資料集
# 假設已下載 enron_spam_data.csv
df = pd.read_csv('enron_spam_data.csv')

# 3. 資料探索 (EDA)
print("資料集形狀:", df.shape)
print("\n資料集前5筆:\n", df.head())
print("\n資料集資訊:\n")
print(df.info())
print("\n目標變數分布:\n", df['Spam/Ham'].value_counts())
print("\n缺失值統計:\n", df.isnull().sum())

# 視覺化類別分布
plt.figure(figsize=(8, 6))
df['Spam/Ham'].value_counts().plot(kind='bar')
plt.title('Spam vs Ham Distribution')
plt.xlabel('Category')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.savefig('spam_ham_distribution.png')
plt.close()

# 4. 文本預處理
def preprocess_text(text):
    '''
    文本預處理函數
    - 轉換為小寫
    - 移除特殊字符和數字
    - 移除停用詞
    - 詞幹提取 (Stemming)
    '''
    if pd.isna(text):
        return ""

    # 轉小寫
    text = text.lower()

    # 移除特殊字符和數字
    text = re.sub(r'[^a-zA-Z\s]', '', text)

    # 分詞
    words = text.split()

    # 移除停用詞
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]

    # 詞幹提取
    stemmer = PorterStemmer()
    words = [stemmer.stem(word) for word in words]

    return ' '.join(words)

# 合併 Subject 和 Message 欄位
df['text'] = df['Subject'].fillna('') + ' ' + df['Message'].fillna('')

# 應用預處理
print("\n開始文本預處理...")
df['cleaned_text'] = df['text'].apply(preprocess_text)

# 5. 特徵工程 - TF-IDF 向量化
print("\n開始 TF-IDF 特徵提取...")
tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
X = tfidf.fit_transform(df['cleaned_text'])

# 將標籤轉換為數值 (spam=1, ham=0)
df['label'] = df['Spam/Ham'].map({'spam': 1, 'ham': 0})
y = df['label']

print(f"特徵矩陣形狀: {X.shape}")
print(f"標籤分布:\n{y.value_counts()}")

# 6. 資料切分 (訓練集與測試集)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\n訓練集大小: {X_train.shape[0]}")
print(f"測試集大小: {X_test.shape[0]}")

# 7. 模型訓練與評估

# 7.1 Naive Bayes
print("\n========== Naive Bayes ==========")
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)
nb_pred = nb_model.predict(X_test)
nb_pred_proba = nb_model.predict_proba(X_test)[:, 1]

print(f"Accuracy: {accuracy_score(y_test, nb_pred):.4f}")
print(f"Precision: {precision_score(y_test, nb_pred):.4f}")
print(f"Recall: {recall_score(y_test, nb_pred):.4f}")
print(f"F1-Score: {f1_score(y_test, nb_pred):.4f}")
print(f"ROC-AUC: {roc_auc_score(y_test, nb_pred_proba):.4f}")

# 7.2 Logistic Regression
print("\n========== Logistic Regression ==========")
lr_model = LogisticRegression(max_iter=1000, random_state=42)
lr_model.fit(X_train, y_train)
lr_pred = lr_model.predict(X_test)
lr_pred_proba = lr_model.predict_proba(X_test)[:, 1]

print(f"Accuracy: {accuracy_score(y_test, lr_pred):.4f}")
print(f"Precision: {precision_score(y_test, lr_pred):.4f}")
print(f"Recall: {recall_score(y_test, lr_pred):.4f}")
print(f"F1-Score: {f1_score(y_test, lr_pred):.4f}")
print(f"ROC-AUC: {roc_auc_score(y_test, lr_pred_proba):.4f}")

# 7.3 Random Forest
print("\n========== Random Forest ==========")
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
rf_pred_proba = rf_model.predict_proba(X_test)[:, 1]

print(f"Accuracy: {accuracy_score(y_test, rf_pred):.4f}")
print(f"Precision: {precision_score(y_test, rf_pred):.4f}")
print(f"Recall: {recall_score(y_test, rf_pred):.4f}")
print(f"F1-Score: {f1_score(y_test, rf_pred):.4f}")
print(f"ROC-AUC: {roc_auc_score(y_test, rf_pred_proba):.4f}")

# 8. 視覺化結果

# 8.1 混淆矩陣 (以 Logistic Regression 為例)
cm = confusion_matrix(y_test, lr_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Ham', 'Spam'],
            yticklabels=['Ham', 'Spam'])
plt.title('Confusion Matrix - Logistic Regression')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.savefig('confusion_matrix.png')
plt.close()

# 8.2 ROC 曲線比較
plt.figure(figsize=(10, 8))

# Naive Bayes ROC
fpr_nb, tpr_nb, _ = roc_curve(y_test, nb_pred_proba)
plt.plot(fpr_nb, tpr_nb, label=f'Naive Bayes (AUC={roc_auc_score(y_test, nb_pred_proba):.4f})')

# Logistic Regression ROC
fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_pred_proba)
plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC={roc_auc_score(y_test, lr_pred_proba):.4f})')

# Random Forest ROC
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_pred_proba)
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC={roc_auc_score(y_test, rf_pred_proba):.4f})')

# 隨機猜測基準線
plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves Comparison')
plt.legend()
plt.grid(True)
plt.savefig('roc_curves.png')
plt.close()

# 8.3 模型性能比較
models = ['Naive Bayes', 'Logistic Regression', 'Random Forest']
accuracies = [
    accuracy_score(y_test, nb_pred),
    accuracy_score(y_test, lr_pred),
    accuracy_score(y_test, rf_pred)
]
f1_scores = [
    f1_score(y_test, nb_pred),
    f1_score(y_test, lr_pred),
    f1_score(y_test, rf_pred)
]

x = np.arange(len(models))
width = 0.35

fig, ax = plt.subplots(figsize=(10, 6))
ax.bar(x - width/2, accuracies, width, label='Accuracy')
ax.bar(x + width/2, f1_scores, width, label='F1-Score')

ax.set_xlabel('Models')
ax.set_ylabel('Scores')
ax.set_title('Model Performance Comparison')
ax.set_xticks(x)
ax.set_xticklabels(models)
ax.legend()
ax.grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('model_comparison.png')
plt.close()

# 9. 交叉驗證
print("\n========== 交叉驗證 (5-Fold) ==========")
cv_scores_nb = cross_val_score(nb_model, X, y, cv=5, scoring='f1')
cv_scores_lr = cross_val_score(lr_model, X, y, cv=5, scoring='f1')
cv_scores_rf = cross_val_score(rf_model, X, y, cv=5, scoring='f1')

print(f"Naive Bayes CV F1-Score: {cv_scores_nb.mean():.4f} (+/- {cv_scores_nb.std():.4f})")
print(f"Logistic Regression CV F1-Score: {cv_scores_lr.mean():.4f} (+/- {cv_scores_lr.std():.4f})")
print(f"Random Forest CV F1-Score: {cv_scores_rf.mean():.4f} (+/- {cv_scores_rf.std():.4f})")

# 10. 預測新郵件範例
def predict_email(text, model, vectorizer):
    '''
    預測單一郵件是否為垃圾郵件
    '''
    cleaned = preprocess_text(text)
    vectorized = vectorizer.transform([cleaned])
    prediction = model.predict(vectorized)[0]
    probability = model.predict_proba(vectorized)[0]

    return "Spam" if prediction == 1 else "Ham", probability

# 測試範例
test_email = "Congratulations! You've won a $1000 gift card. Click here to claim now!"
result, prob = predict_email(test_email, lr_model, tfidf)
print(f"\n測試郵件: {test_email}")
print(f"預測結果: {result}")
print(f"機率: Spam={prob[1]:.4f}, Ham={prob[0]:.4f}")

print("\n專案完成！")

if __name__ == '__main__':
    print("腳本執行完畢")

# --- 模擬測試程式碼 ---

# 1. 定義新的測試郵件
new_emails = [
    # 正常郵件 (預期：Ham / 0)
    "I am requesting a vacation from August 10th to August 15th. I have attached the formal request form.",
    "Hey Mike, can you meet at 3 PM for coffee? It's about the project status.",

    # 垃圾郵件 (預期：Spam / 1)
    "Congratulations!!! Your credit card has been approved for a FREE vacation to the BAHAMAS! Reply YES to 4455 now!",
    "Attention: Important security update. Click this link immediately to avoid losing access to your money: http://badurl.com/login"
]

# 2. 清理新郵件：必須使用訓練時使用的 *clean_text* 函數
cleaned_new_emails = [preprocess_text(email) for email in new_emails]
print("--- 已清理郵件範例 ---")
print(cleaned_new_emails[2]) # 觀察垃圾郵件清理後的結果

# 3. 特徵轉換：必須使用訓練時使用的 *tfidf* 進行轉換
new_emails_vec = tfidf.transform(cleaned_new_emails)
print(f"\n轉換後特徵矩陣形狀: {new_emails_vec.shape}")

# 4. 進行預測
predictions = lr_model.predict(new_emails_vec)

# 5. 輸出結果
print("\n--- 預測結果 ---")
for email, prediction in zip(new_emails, predictions):
    status = "垃圾郵件 (SPAM)" if prediction == 1 else "正常郵件 (HAM)"

    # 為了顯示整潔，只印出郵件的前 60 個字元
    display_email = email.replace('\n', ' ')
    print(f"郵件: '{display_email[:60]}...'")
    print(f" -> 預測結果: {status}\n")